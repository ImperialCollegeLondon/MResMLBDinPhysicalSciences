{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 2. HEP Python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "---\n",
    "\n",
    "For this exercise you will be using the data located at: http://www.hep.ph.ic.ac.uk/~arichard/pgtasks/data_exercise1.root\n",
    "\n",
    "In this Root file you should find two things:\n",
    "\n",
    "1) A histogram of normally distributed random data labelled \"normal\"\n",
    "\n",
    "2) A TTree labelled \"parabola_data\" containing two branches representing the \"x\" and \"y\" points for a scatter plot of a parabola\n",
    "\n",
    "Your task is to:\n",
    "* Extract the histogram and the parabola data and plot them\n",
    "* Using iminuit fit these data with appropriate models using method of least squares and log-likelihood\n",
    "* Extract the model parameters and their errors\n",
    "* Plot data and model on the same figure\n",
    "* calculate a $\\chi^{2}$ goodness of fit statistic and associated P-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.\n",
    "---\n",
    "\n",
    "For this exercise we will be using some ATLAS $Z\\mu\\mu$ data from the CERN opendata project.\n",
    "\n",
    "The data file can be located here: http://opendata.cern.ch/record/3821/files/mc_147771.Zmumu.root\n",
    "\n",
    "Your task is:\n",
    "\n",
    "* To reconstruct the Z Bosons by plotting the invariant mass spectrum of the two muons.\n",
    "* Fit the invariant mass spectrum\n",
    "* From your fit extract the value of the Z mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.\n",
    "---\n",
    "\n",
    "This time we will look at a real world dataset (albeit Monte Carlo data). A CMS Higgs dataset is available at the following location:\n",
    "\n",
    "#### /vols/cms/arichard/PGLectures/VH.root\n",
    "\n",
    "The task is as follows:\n",
    "* Using uproot, explore the dataset\n",
    "* Plot a histogram of the di-photon invariant mass\n",
    "* Attempt to fit a Breit-Wigner distribution to it using both a least-squares and log-likelihood method in minuit. You may additionally use the scipy.optimize.curve_fit function if you like\n",
    "* What does the result tell you?\n",
    "* Using literature determine a better model(s)\n",
    "* Fit these new model(s) using a least-squares and log-likelihood approach in minuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.\n",
    "---\n",
    "\n",
    "In this exersice we will be using the Iris toy dataset supplied with scikit-learn to help us take the first steps in machine learning. To load the dataset we need to import the loader function from scikit-learn as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "```\n",
    "The feature data for training and testing is stored in the data attribute while the correct classifications are stored in the target attribute. There are other useful attributes, to see a complete list do the following:\n",
    "```python\n",
    "print(iris.keys())\n",
    "```\n",
    "To get a better understanding of the dataset and the task at hand print out the accompanying description with:\n",
    "```python\n",
    "print(iris.DESCR)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try training a SVC classifier on this data:\n",
    "* Split the dataset (not forgetting the target data) into both train and test pairs\n",
    "* Using a simple SVC estimator fit your training set\n",
    "* What score do you get using your test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at some other metrics. Firstly import the following helper functions:\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "```\n",
    "\n",
    "Now do the following (you may need to consult the documentation for the API):\n",
    "* Use the svc estimator to get the predictions from your test dataset.\n",
    "* Print out the RMS error between your predictions and the true targets.\n",
    "* Print out the confusion Matrix and understand what it's telling you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can plot distributions of some of the features or combination thereof and compare our predictions to the truth/target values.\n",
    "* Try plotting the distribution of petal length from the test dataset explicitly treating your predictions as the labels for three distinct series.\n",
    "* Plot also the same distribution with the target values as the series for comparison.\n",
    "* Repeat for any other distributions you like e.g. scatter plot of sepal width vs petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If your feeling confident you could try using the seaborn package to do this in an easier way. Since this package was not introduced in the lectures you will have to read the documentation for yourself, otherwise stick to matplotlib.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.\n",
    "---\n",
    "\n",
    "In this exercise we will be using the Boston house prices dataset which is a regression problem. You will need to load the dataset as before using:\n",
    "```python\n",
    "from sklearn.datasets import load_boston\n",
    "```\n",
    "\n",
    "As before try printing out the `keys()` and the `DESCR` from this dataset to see what we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we shall explore the data:\n",
    "* Plot some of the feature distributions to get a feel for the data.\n",
    "* Print or plot the correlation between the different features. Include the target to see how the features correlate with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting only the features that correlate most highly with the target variable:\n",
    "* Split the dataset into training and testing samples\n",
    "* Use a LinearRegression estimator to fit the training sample. You will need the following import\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "```\n",
    "* What score do you get using this trained estimator on the testing data\n",
    "* Use the trained estimator to get the predicted values of house prices\n",
    "* What is the RMS error between your predictions and the True target values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally:\n",
    "* Make a plot of the predicted house prices against the true values.\n",
    "* Comment on it's shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.\n",
    "---\n",
    "\n",
    "If you have reached this point and you would like an extra challenge, you can try and attempt the [ATLAS Higgs Machine Learning Challenge](http://opendata.cern.ch/record/328)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
